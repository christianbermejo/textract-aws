AWSTemplateFormatVersion: 2010-09-09
Transform:
- AWS::Serverless-2016-10-31

Parameters:
  Environment:
    Description:    Please specify the target environment.
    Type:           String
    Default:        "staging"
    AllowedValues:
      - prod
      - staging
      - dev
      - qa
  # Domain: example-com
  ProjectName:
    Type: String
    Description: "The project name"
    MinLength: 4
    MaxLength: 253
    AllowedPattern: "[a-z0-9]+[-.a-z0-9]*"
    ConstraintDescription: |
      Provide a valid project name using only lowercase letters, 
      numbers, and dash (-).
    Default: aws-txt
  # Domain: example.com
  DomainName:
    Type: String
    Description: "The base domain name for the web site (no 'www')"
    MinLength: 4
    MaxLength: 253
    AllowedPattern: "[a-z0-9]+[-.a-z0-9]*(\\.[a-z][a-z]+)+"
    ConstraintDescription: |
      Provide a valid domain name using only lowercase letters, 
      numbers, ., and dash (-)
    Default: 'aws-txt.com'
  AppName:
    Description: Name of the application deploying for AWS Project
    Type: String
    Default: AWSTXT
  DOMAINNAME:
    Description: Name for the Amazon ES domain that this template will create. Domain names must start with a lowercase letter and must be between 3 and 28 characters. Valid characters are a-z (lowercase only), 0-9.
    Type: String
    Default: resumecrawlocrapp
  CognitoAdminEmail:
    Type: String
    Default: cybermejo@gmail.com
    AllowedPattern: '^\w+([\.-]?\w+)*@\w+([\.-]?\w+)*(\.\w{2,3})+$'
    Description: E-mail address of the Cognito admin name
  OriginProtocolPolicy:
    Description:    CloudFront Origin Protocol Policy to apply to your origin.
    Type:           String
    Default:        "http-only"
    AllowedValues:
      - http-only
      - match-viewer
      - https-only
  OriginKeepaliveTimeout:
    Description:    You can create a custom keep-alive timeout. All timeout units are in seconds. The default keep-alive timeout is 5 seconds, but you can configure custom timeout lengths. The minimum timeout length is 1 second; the maximum is 60 seconds.
    Type:           String
    Default:        "60"

  OriginReadTimeout:
    Description:    You can create a custom origin read timeout. All timeout units are in seconds. The default origin read timeout is 30 seconds, but you can configure custom timeout lengths. The minimum timeout length is 4 seconds; the maximum is 60 seconds.
    Type:           String
    Default:        "30"
  PreExistingCodeCommitRepository:
    Description: |
      Optional pre-existing CodeCommit repository. 
      Leave empty to have repository created and managed by this stack.
    Type: String
    Default: ""
  PreExistingCodePipelineBucket:
    Description: |
      Optional name of pre-existing CodePipeline artifact bucket.
      Leave empty to have CodePipeline bucket created and managed by this stack.
    Type: String
    Default: ""

# Enable blue/green deployments using this Globals section. For instructions, see the AWS CodeStar User Guide:
# https://docs.aws.amazon.com/codestar/latest/userguide/how-to-modify-serverless-project.html?icmpid=docs_acs_rm_tr
#
# Globals:
#   Function:
#     AutoPublishAlias: live
#     DeploymentPreference:
#       Enabled: true
#       Type: Canary10Percent5Minutes

Conditions:
# Conditions for CodeCommit
  NeedsNewCodePipelineBucket: !Equals [!Ref PreExistingCodePipelineBucket, ""]
  NeedsNewCodeCommitRepository:
    !Equals [!Ref PreExistingCodeCommitRepository, ""]

Mappings:
  # Lambda source code mapping
  SourceCode:
    General:
      S3Bucket: "solutions"
      KeyPrefix: "centralized-logging/v2.2.0"
  #  This is just the Amazon Linux AMI:
  AmazonLinuxAMI:
    us-east-1: # Virginia
      AMI: ami-a4c7edb2
    us-east-2: # Ohio
      AMI: ami-8a7859ef
    us-west-1: # North California
      AMI: ami-327f5352
    us-west-2: # Oregon
      AMI: ami-6df1e514
    eu-west-1: # Ireland
      AMI: ami-d7b9a2b1
    eu-west-2: # London
      AMI: ami-ed100689
    eu-central-1: # Frankfurt
      AMI: ami-82be18ed
    sa-east-1: # Sao Paulo
      AMI: ami-87dab1eb
    ap-southeast-1: # Singapore
      AMI: ami-77af2014
    ap-southeast-2: # Sydney
      AMI: ami-10918173
    ap-northeast-1: # Tokyo
      AMI: ami-3bd3c45c
    ap-northeast-2: # Seoul
      AMI: ami-e21cc38c
    ca-central-1: # Canada
      AMI: ami-a7aa15c3
    ap-south-1: # Mumbai
      AMI: ami-47205e28

Resources:

### LAYER RESOURCES
  MyLayer:
    Type: AWS::Serverless::LayerVersion
    Properties:
        LayerName: Boto3
        Description: My Lambda Layer with Deserialize Function, Objectpath Module & AWS Regions JSON
        ContentUri: lambda-layers/boto3-layer.zip
        CompatibleRuntimes:
            - python3.6
            - python3.7

  HelperLayer:
    Type: AWS::Serverless::LayerVersion
    Properties:
        LayerName: HelperLayer
        Description: Helper Layer
        ContentUri: lambda/helper
        CompatibleRuntimes:
            - python3.6
            - python3.7

  TextractorLayer:
    Type: AWS::Serverless::LayerVersion
    Properties:
        LayerName: Textractor
        Description: Textractor Layer
        ContentUri: lambda/textractor
        CompatibleRuntimes:
            - python3.6
            - python3.7

## SQS Queues
  DLQ:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'DLQ'
      VisibilityTimeout: 30
      MessageRetentionPeriod: 1209600

  SyncJobs:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'SyncJobs'
      VisibilityTimeout: 30
      MessageRetentionPeriod: 1209600
      RedrivePolicy:
        deadLetterTargetArn : !GetAtt ["DLQ", "Arn"]
        maxReceiveCount : 50

  AsyncJobs:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'AsyncJobs'
      VisibilityTimeout: 30
      MessageRetentionPeriod: 1209600
      RedrivePolicy:
        deadLetterTargetArn : !GetAtt ["DLQ", "Arn"]
        maxReceiveCount : 50

  JobResults:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: 'JobResults'
      VisibilityTimeout: 900
      MessageRetentionPeriod: 1209600
      RedrivePolicy:
        deadLetterTargetArn : !GetAtt ["DLQ", "Arn"]
        maxReceiveCount : 50

### SNS TOPICS
  JobCompletion:
    Type: "AWS::SNS::Topic"
    Properties:
      Subscription:
        - Endpoint: !GetAtt ["JobResults", "Arn"]
          Protocol: sqs

## IAM ROLE FOR TEXTRACT SERVICES
  TextractServiceRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Action:
              - "sts:AssumeRole"
            Principal:
              Service:
                - "textract.amazonaws.com"
      Policies:
        - PolicyName: 'TextractServiceRolePolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'sns:Publish'
                # Get TopicARN of SNS
                Resource: !Ref JobCompletion

### START OF CI/CD RESOURCES
  # Git repository
  CodeCommitRepository:
    Condition: NeedsNewCodeCommitRepository
    Type: "AWS::CodeCommit::Repository"
    Properties:
      RepositoryDescription: !Sub "Git repository for ${DomainName}"
      RepositoryName: !Ref ProjectName
    DeletionPolicy: Delete

  # Role for CodeBuild
  CodeBuildServiceRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - "codebuild.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: "/"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AWSCodeCommitFullAccess"
        - "arn:aws:iam::aws:policy/AmazonS3FullAccess"
        - "arn:aws:iam::aws:policy/CloudWatchLogsFullAccess"
        - "arn:aws:iam::aws:policy/AWSCodeBuildDeveloperAccess"

  # CodeBuild Project
  CodeBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Ref ProjectName
      ServiceRole: !GetAtt CodeBuildServiceRole.Arn
      Artifacts:
        Type: NO_ARTIFACTS
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/nodejs:8.11.0
        EnvironmentVariables:
          - Name: SITE_BUCKET
            Value: !Ref S3Static
      Source:
        Location: !Sub "https://git-codecommit.${AWS::Region}.amazonaws.com/v1/repos/${ProjectName}"
        Type: CODECOMMIT
        BuildSpec: |
          version: 0.1
          phases:
            install:
              commands:
                - sudo npm install
            build:
              commands:
                - sudo node_modules/.bin/browserify public/index.js -i browserify -s app > public/app.js
            post_build:
              commands:
                - aws s3 sync public/ s3://$SITE_BUCKET/ --delete
      TimeoutInMinutes: 10

# Role for CodePipeline
  CodePipelineRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "codepipeline.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: "/"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AWSCodeCommitFullAccess"
        - "arn:aws:iam::aws:policy/AmazonS3FullAccess"
        - "arn:aws:iam::aws:policy/CloudWatchLogsFullAccess"
        - "arn:aws:iam::aws:policy/AWSCodeBuildDeveloperAccess"

  # Bucket for CodePipeline artifact storage: codepipeline.example.com
  CodePipelineBucket:
    Condition: NeedsNewCodePipelineBucket
    Type: "AWS::S3::Bucket"
    Properties:
      BucketName: !Sub "codepipeline-${ProjectName}"
      VersioningConfiguration:
        Status: Enabled
    DeletionPolicy: Delete

  # CodePipeline for the static website
  CodePipeline:
    Type: "AWS::CodePipeline::Pipeline"
    Properties:
      Name: !Sub "${ProjectName}-codepipeline"
      ArtifactStore:
        Type: S3
        Location:
          !If [
            NeedsNewCodePipelineBucket,
            !Ref CodePipelineBucket,
            !Ref PreExistingCodePipelineBucket,
          ]
      RestartExecutionOnUpdate: false
      RoleArn: !GetAtt CodePipelineRole.Arn
      Stages:
        - Name: Source
          Actions:
            - Name: SourceAction
              ActionTypeId:
                Category: Source
                Owner: AWS
                Provider: CodeCommit
                Version: '1'
              Configuration:
                RepositoryName:
                  !If [
                    NeedsNewCodeCommitRepository,
                    !Ref ProjectName,
                    !Ref PreExistingCodeCommitRepository,
                  ]
                BranchName: master
              OutputArtifacts:
                - Name: SiteSource
              RunOrder: 1
        - Name: Build
          Actions:
            - Name: InvokeAction
              InputArtifacts:
                - Name: SiteSource
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              Configuration:
                ProjectName: !Ref ProjectName
              OutputArtifacts:
                - Name: SiteContent
              RunOrder: 1

### END OF CI/CD RESOURCES

### LAMBDA FUNCTIONS AND ITS ROLES
# S3 Processor
  S3Processor:
    Type: AWS::Serverless::Function
    Properties:
      Runtime: python3.6
      CodeUri: lambda/s3processor
      Handler: lambda_function.lambda_handler
      Policies:
        Statement:
        # grant read write data to documentsTable
          -
            Sid: "s3processor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${documentsTable}'
          -
            Sid: "syncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ SyncJobs, Arn ]
          -
            Sid: "asyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ AsyncJobs, Arn ]
      Environment:
        Variables:
          SYNC_QUEUE_URL: !Ref SyncJobs
          ASYNC_QUEUE_URL: !Ref AsyncJobs
          DOCUMENTS_TABLE: !Ref documentsTable
          OUTPUT_TABLE: !Ref outputTable
      Layers:
        - !Ref HelperLayer
      Events:
        S3ProcessorEvent:
          Type: S3
          Properties:
            Bucket: !Ref S3
            Events:
              - s3:ObjectCreated:*

# S3 Batch Processor
  S3BatchProcessor:
    Type: AWS::Serverless::Function
    Properties:
      Runtime: python3.6
      CodeUri: lambda/s3batchprocessor
      Handler: lambda_function.lambda_handler
      ReservedConcurrentExecutions: 1
      Policies:
        Statement:
        # grant read write data to documentsTable
          -
            Sid: "s3batchprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${documentsTable}'
      Environment:
        Variables:
          DOCUMENTS_TABLE: !Ref documentsTable
          OUTPUT_TABLE: !Ref outputTable
      Layers:
        - !Ref HelperLayer

# THIS CAN ALSO BE IMPLEMENTED AS AN IAM POLICY TO S3BATCHOPERATIONS ROLE
# LET'S SEE IF IT WORKS
  S3BatchPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:invokeFunction
      SourceAccount: !Ref 'AWS::AccountId'
      FunctionName: !Ref 'S3BatchProcessor'
      SourceArn: !GetAtt [ S3BatchOperationsRole, Arn ]
      Principal: batchoperations.s3.amazonaws.com

# Document Processor for Sync/Async Pipeline
  TaskProcessor:
    Type: AWS::Serverless::Function
    Properties:
      Runtime: python3.6
      CodeUri: lambda/documentprocessor
      Handler: lambda_function.lambda_handler
      Policies:
        Statement:
        # grant read write data to documentsTable
          -
            Sid: "taskprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${documentsTable}'
          -
            Sid: "tasksyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ SyncJobs, Arn ]
          -
            Sid: "taskasyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ AsyncJobs, Arn ]
      Environment:
        Variables:
          SYNC_QUEUE_URL: !Ref SyncJobs
          ASYNC_QUEUE_URL: !Ref AsyncJobs
      Layers:
        - !Ref HelperLayer
      Events:
        DynamoDBEvent:
          Type: DynamoDB
          Properties:
            Stream: !GetAtt [ documentsTable, StreamArn ]
            StartingPosition: TRIM_HORIZON

# Sync Jobs Processor (Process jobs using sync APIs)
  SyncProcessor:
    Type: AWS::Serverless::Function
    Properties:
      Runtime: python3.6
      CodeUri: lambda/syncprocessor
      Handler: lambda_function.lambda_handler
      ReservedConcurrentExecutions: 1
      Timeout: 25
      Policies:
        Statement:
        # grant read write to content bucket
          -
            Sid: "contentbucket"
            Effect: Allow
            Action:
              - s3:*
            Resource: !Sub "arn:aws:s3:::${S3}/*"
        # add role policy for textract 
          -
            Sid: "textract"
            Effect: Allow
            Action:
              - textract:*
            Resource: "*"
        # grant read write data to outputTable
          -
            Sid: "syncoutputprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${outputTable}'
        
        # grant read write data to documentsTable
          -
            Sid: "syncdocumentsprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${documentsTable}'
          -
            Sid: "tasksyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ SyncJobs, Arn ]
          -
            Sid: "taskasyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ AsyncJobs, Arn ]
      Environment:
        Variables:
          DOCUMENTS_TABLE: !Ref documentsTable
          OUTPUT_TABLE: !Ref outputTable
          AWS_DATA_PATH: 'models'
          ESDOMAIN: !GetAtt ElasticsearchDomain.DomainEndpoint
      Layers:
        - !Ref HelperLayer
        - !Ref TextractorLayer
      Events:
        SQSCheckEvent:
          Type: SQS
          Properties:
            Queue: !GetAtt [ SyncJobs , Arn]
            BatchSize: 1

# Async Job Processor (Start jobs using Async APIs)
  ASyncProcessor:
    Type: AWS::Serverless::Function
    Properties:
      Runtime: python3.6
      CodeUri: lambda/asyncprocessor
      Handler: lambda_function.lambda_handler
      ReservedConcurrentExecutions: 1
      Timeout: 60
      Policies:
        Statement:
        # grant read to content bucket
          -
            Sid: "contentbucket"
            Effect: Allow
            Action:
              - s3:GetObject*
              - s3:GetBucket*
              - s3:List*
            Resource: !Sub "arn:aws:s3:::${S3}/*"
        # add role policy for textract
          -
            Sid: "textract"
            Effect: Allow
            Action:
              - textract:*
            Resource: "*"
          -
            Sid: "iamPassRole"
            Effect: Allow
            Action:
              - iam:PassRole
            Resource: !GetAtt [ TextractServiceRole, Arn ]
        # grant read write data to outputTable
          -
            Sid: "syncoutputprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${outputTable}'
        
        # grant read write data to documentsTable
          -
            Sid: "syncdocumentsprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${documentsTable}'
          -
            Sid: "tasksyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ SyncJobs, Arn ]
          -
            Sid: "taskasyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ AsyncJobs, Arn ]
          # Grant ConsumeMessages from ASync SQS
          -
            Sid: "asyncConsumeMessage"
            Effect: Allow
            Action:
              - sqs:ChangeMessageVisibility
              - sqs:DeleteMessage
              - sqs:ReceiveMessage
              - sqs:GetQueueAttributes
              - sqs:GetQueueUrl
            Resource: !GetAtt [ AsyncJobs, Arn ]
      Environment:
        Variables:
          ASYNC_QUEUE_URL: !Ref AsyncJobs
          SNS_TOPIC_ARN : !Ref JobCompletion
          SNS_ROLE_ARN : !GetAtt [ TextractServiceRole, Arn ]
          AWS_DATA_PATH: 'models'
      Layers:
        - !Ref HelperLayer
      Events:
        SNSCheckEvent:
          Type: SNS
          Properties:
            Topic: !Ref  JobCompletion

  # EventRule for Async Processor
  EventRule:
    Type: AWS::Events::Rule
    Properties:
      Description: EventRule
      ScheduleExpression: 'rate(2 minutes)'
      Targets:
        - Id: ASyncEventRule
          Arn: !GetAtt [ ASyncProcessor, Arn ]

# Async Job Results Processor
  JobResultProcessor:
    Type: AWS::Serverless::Function
    Properties:
      Runtime: python3.6
      CodeUri: lambda/jobresultprocessor
      Handler: lambda_function.lambda_handler
      ReservedConcurrentExecutions: 50
      MemorySize: 2000
      Timeout: 900
      Policies:
        Statement:
        # grant read write to content bucket
          -
            Sid: "contentbucket"
            Effect: Allow
            Action:
              - s3:*
            Resource: !Sub "arn:aws:s3:::${S3}/*"
        # add role policy for textract 
          -
            Sid: "textract"
            Effect: Allow
            Action:
              - textract:*
            Resource: "*"
        # grant read write data to outputTable
          -
            Sid: "syncoutputprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${outputTable}'
        
        # grant read write data to documentsTable
          -
            Sid: "syncdocumentsprocessor"
            Effect: Allow
            Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:GetRecords
              - dynamodb:GetShardIterator
              - dynamodb:PutItem
              - dynamodb:Query
              - dynamodb:Scan
              - dynamodb:UpdateItem
            Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${documentsTable}'
          -
            Sid: "tasksyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ SyncJobs, Arn ]
          -
            Sid: "taskasyncjobqueue"
            Effect: Allow
            Action:
              - sqs:SendMessage
              - sqs:SendMessageBatch
              - sqs:GetQueueAttributes
              - sqs:GetQueueURL
            Resource: !GetAtt [ AsyncJobs, Arn ]
      Environment:
        Variables:
          DOCUMENTS_TABLE: !Ref documentsTable
          OUTPUT_TABLE: !Ref outputTable
          AWS_DATA_PATH: 'models'
      Layers:
        - !Ref HelperLayer
        - !Ref TextractorLayer
      Events:
        SQSResultsEvent:
          Type: SQS
          Properties:
            Queue: !GetAtt [ JobResults , Arn]
            BatchSize: 1

  ComprehendKeyPhraseAnalysis:
    Properties:
      Description: "Triggered by S3 review upload to the repo bucket and start the key phrase analysis via Amazon Comprehend"
      Handler: comprehend.handler
      MemorySize: 128
      Policies:
        Statement:
          -
            Sid: "comprehend"
            Effect: Allow
            Action:
              - comprehend:*
            Resource: "*"
          -
            Sid: "textract"
            Effect: Allow
            Action:
              - textract:*
            Resource: "*"
          -
            Sid: "s3"
            Effect: Allow
            Action:
              - s3:*
            Resource: !Sub "arn:aws:s3:::${S3}/*"
          -
            Sid: "es"
            Effect: Allow
            Action:
              - es:*
            Resource: "*"
      Environment:
        Variables:
          bucket: !Ref S3
          esDomain: !GetAtt ElasticsearchDomain.DomainEndpoint
          
      Runtime: python3.6
      Timeout: 300
      Layers: 
        - !Ref MyLayer
    Type: AWS::Serverless::Function

## IAM ROLE FOR S3 BATCH OPERATIONS SERVICES
  S3BatchOperationsRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Action:
              - "sts:AssumeRole"
            Principal:
              Service:
                - "batchoperations.s3.amazonaws.com"
      Policies:
        - PolicyName: 'S3BatchOperationsRolePolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - lambda:*
                Resource: '*'

  # Create bucket to store pictures
  S3:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: PublicReadWrite
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders:
              - "*"
            AllowedMethods:
              - GET
              - POST
              - PUT
              - DELETE
              - HEAD
            AllowedOrigins:
              - "*"

  # Create bucket for S3 static content
  S3Static:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: PublicRead
      WebsiteConfiguration:
        IndexDocument: index.html
        ErrorDocument: error.html
    DeletionPolicy: Retain

  # Create bucket for inventory and logs
  InventoryAndLogsBucket:
    Type: AWS::S3::Bucket

  # Create bucket policy for S3 static
  BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      PolicyDocument:
        Id: S3StaticPolicy
        Version: 2012-10-17
        Statement:
          - Sid: PublicReadForGetBucketObjects
            Effect: Allow
            Principal: '*'
            Action: 's3:GetObject'
            Resource: !Join 
              - ''
              - - 'arn:aws:s3:::'
                - !Ref S3Static
                - /*
      Bucket: !Ref S3Static

### IAM Policy for Read and Write Access for S3 Batch Operations Role
  GetS3ContentPolicy:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: S3ContentPolicy
      PolicyDocument:
        Statement:
          - Effect: Allow
            Action:
              - 's3:ListBucket'
            Resource:
              - !Join 
                - ''
                - - 'arn:aws:s3:::'
                  - !Ref InventoryAndLogsBucket
          - Effect: Allow
            Action:
              - 's3:*Object'
            Resource:
              - !Join 
                - ''
                - - 'arn:aws:s3:::'
                  - !Ref InventoryAndLogsBucket
                  - /*
      Roles:
        - !Ref S3BatchOperationsRole

## DynamoDB Tables as Streams to trigger Lambda functions to write to SQS queue
  documentsTable:
    Type: 'AWS::DynamoDB::Table'
    Properties: 
      AttributeDefinitions: 
        - 
          AttributeName: "documentId"
          AttributeType: "S"
      KeySchema: 
        - 
          AttributeName: "documentId"
          KeyType: "HASH"
      StreamSpecification:
        StreamViewType: 'NEW_IMAGE'
      TableName: "DocumentsTable"
      BillingMode: PAY_PER_REQUEST

  outputTable: 
    Type: "AWS::DynamoDB::Table"
    Properties: 
      AttributeDefinitions: 
        - 
          AttributeName: "documentId"
          AttributeType: "S"
        - 
          AttributeName: "outputType"
          AttributeType: "S"
      KeySchema: 
        - 
          AttributeName: "documentId"
          KeyType: "HASH"
        - 
          AttributeName: "outputType"
          KeyType: "RANGE"
      TableName: "OutputTable"

  # CloudFront Distribution
  myDistribution:
    Type: AWS::CloudFront::Distribution
    Properties:
      DistributionConfig:
        Comment: 'CloudFront Distribution to ALB and S3 Origin'
        Origins:
        - Id: myS3Origin
          DomainName: !GetAtt [ S3Static, DomainName ]
          CustomOriginConfig:
            HTTPPort: 80
            HTTPSPort: 443
            OriginProtocolPolicy: !Ref 'OriginProtocolPolicy'
            OriginKeepaliveTimeout: !Ref 'OriginKeepaliveTimeout'
            OriginReadTimeout: !Ref 'OriginReadTimeout'
            OriginSSLProtocols:
              - TLSv1
              - TLSv1.1
              - TLSv1.2
              - SSLv3
        Enabled: true
        DefaultRootObject: index.html
        DefaultCacheBehavior:
          AllowedMethods:
          - DELETE
          - GET
          - HEAD
          - OPTIONS
          - PATCH
          - POST
          - PUT
          TargetOriginId: myS3Origin
          ForwardedValues:
            QueryString: false
            Cookies:
              Forward: none
          ViewerProtocolPolicy: allow-all
        PriceClass: PriceClass_200
        Restrictions:
          GeoRestriction:
            RestrictionType: whitelist
            Locations:
            - AQ
            - CV
            - PH
        ViewerCertificate:
          CloudFrontDefaultCertificate: true

  TestS3BucketEventPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:invokeFunction
      SourceAccount: !Ref 'AWS::AccountId'
      FunctionName: !Ref 'ComprehendKeyPhraseAnalysis'
      SourceArn: !GetAtt
        - S3
        - Arn
      Principal: s3.amazonaws.com

  ApplyNotificationFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: S3BucketNotificationPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: AllowBucketNotification
                Effect: Allow
                Action: s3:PutBucketNotification
                Resource:
                  - !Sub 'arn:aws:s3:::${S3}'
                  - !Sub 'arn:aws:s3:::${S3}/*'
                  
                  
  ApplyBucketNotificationFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Dummy function, just logs the received event
      Handler: index.handler
      Runtime: python2.7
      Role: !GetAtt 'ApplyNotificationFunctionRole.Arn'
      Timeout: 240
      Code:
        ZipFile: |
          import boto3
          import logging
          import json
          import cfnresponse

          s3Client = boto3.client('s3')
          logger = logging.getLogger()
          logger.setLevel(logging.DEBUG)

          def addBucketNotification(bucketName, notificationId, functionArn):
            notificationResponse = s3Client.put_bucket_notification_configuration(
              Bucket=bucketName,
              NotificationConfiguration={
                'LambdaFunctionConfigurations': [
                  {
                    'Id': notificationId,
                    'LambdaFunctionArn': functionArn,
                    'Events': [
                      's3:ObjectCreated:*'
                    ]
                  },
                ]
              }
            )
            return notificationResponse

          def create(properties, physical_id):
            bucketName = properties['S3Bucket']
            notificationId = properties['NotificationId']
            functionArn = properties['FunctionARN']
            response = addBucketNotification(bucketName, notificationId, functionArn)
            logger.info('AddBucketNotification response: %s' % json.dumps(response))
            return cfnresponse.SUCCESS, physical_id

          def update(properties, physical_id):
            return cfnresponse.SUCCESS, None

          def delete(properties, physical_id):
            return cfnresponse.SUCCESS, None

          def handler(event, context):
            logger.info('Received event: %s' % json.dumps(event))

            status = cfnresponse.FAILED
            new_physical_id = None

            try:
              properties = event.get('ResourceProperties')
              physical_id = event.get('PhysicalResourceId')

              status, new_physical_id = {
                'Create': create,
                'Update': update,
                'Delete': delete
              }.get(event['RequestType'], lambda x, y: (cfnresponse.FAILED, None))(properties, physical_id)
            except Exception as e:
              logger.error('Exception: %s' % e)
              status = cfnresponse.FAILED
            finally:
              cfnresponse.send(event, context, status, {}, new_physical_id)

  # Cognito and IAM
  #
  # Creates a user pool in cognito to auth against
  UserPool:
    Type: 'AWS::Cognito::UserPool'
    Properties:
      UserPoolName: !Sub ${DOMAINNAME}_kibana_access
      AutoVerifiedAttributes:
        - email
      MfaConfiguration: 'OFF'
      EmailVerificationSubject: !Ref AWS::StackName
      Schema:
        - Name: name
          AttributeDataType: String
          Mutable: true
          Required: true
        - Name: email
          AttributeDataType: String
          Mutable: false
          Required: true

  # Creates a needed group in Cognito for Kibana access
  UserPoolGroup:
    Type: "AWS::Cognito::UserPoolGroup"
    Properties:
      Description: 'User pool group for Kibana access'
      GroupName: !Sub ${DOMAINNAME}_kibana_access_group
      Precedence: 0
      UserPoolId: !Ref UserPool

  # Creates a User Pool Client to be used by the identity pool
  UserPoolClient:
    Type: 'AWS::Cognito::UserPoolClient'
    Properties:
      ClientName: !Sub ${DOMAINNAME}-client
      GenerateSecret: false
      UserPoolId: !Ref UserPool

  # Creates a federated Identity pool
  # Identity Pool to be also used for S3 uploads
  IdentityPool:
    Type: 'AWS::Cognito::IdentityPool'
    Properties:
      IdentityPoolName: !Sub ${DOMAINNAME}Identity
      AllowUnauthenticatedIdentities: true
      CognitoIdentityProviders:
        - ClientId: !Ref UserPoolClient
          ProviderName: !GetAtt UserPool.ProviderName

  # Create a role for unauthorized access to AWS resources. Very limited access.
  # Only allows users in the previously created Identity Pool
  # To be also used for S3 uploads
  CognitoUnAuthorizedRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Federated: 'cognito-identity.amazonaws.com'
            Action:
              - 'sts:AssumeRoleWithWebIdentity'
            Condition:
              StringEquals:
                'cognito-identity.amazonaws.com:aud': !Ref IdentityPool
              'ForAnyValue:StringLike':
                'cognito-identity.amazonaws.com:amr': unauthenticated
      Policies:
        - PolicyName: 'CognitoUnauthorizedPolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'mobileanalytics:PutEvents'
                  - 'cognito-sync:BulkPublish'
                  - 'cognito-sync:DescribeIdentityPoolUsage'
                  - 'cognito-sync:GetBulkPublishDetails'
                  - 'cognito-sync:GetCognitoEvents'
                  - 'cognito-sync:GetIdentityPoolConfiguration'
                  - 'cognito-sync:ListIdentityPoolUsage'
                  - 'cognito-sync:SetCognitoEvents'
                  - 'cognito-sync:SetIdentityPoolConfiguration'
                Resource: !Sub 'arn:aws:cognito-identity:${AWS::Region}:${AWS::AccountId}:identitypool/${IdentityPool}'
        - PolicyName: 'S3UnauthorizedUploadPolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 's3:*'
                Resource: !Join ["", [ 'arn:aws:s3:::' , !Ref S3 ,'/*']]

  # Create a role for authorized access to AWS resources.
  # Only allows users in the previously created Identity Pool
  CognitoAuthorizedRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Federated: 'cognito-identity.amazonaws.com'
            Action:
              - 'sts:AssumeRoleWithWebIdentity'
            Condition:
              StringEquals:
                'cognito-identity.amazonaws.com:aud': !Ref IdentityPool
              'ForAnyValue:StringLike':
                'cognito-identity.amazonaws.com:amr': authenticated
      Policies:
        - PolicyName: 'CognitoAuthorizedPolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'mobileanalytics:PutEvents'
                  - 'cognito-sync:BulkPublish'
                  - 'cognito-sync:DescribeIdentityPoolUsage'
                  - 'cognito-sync:GetBulkPublishDetails'
                  - 'cognito-sync:GetCognitoEvents'
                  - 'cognito-sync:GetIdentityPoolConfiguration'
                  - 'cognito-sync:ListIdentityPoolUsage'
                  - 'cognito-sync:SetCognitoEvents'
                  - 'cognito-sync:SetIdentityPoolConfiguration'
                  - 'cognito-identity:DeleteIdentityPool'
                  - 'cognito-identity:DescribeIdentityPool'
                  - 'cognito-identity:GetIdentityPoolRoles'
                  - 'cognito-identity:GetOpenIdTokenForDeveloperIdentity'
                  - 'cognito-identity:ListIdentities'
                  - 'cognito-identity:LookupDeveloperIdentity'
                  - 'cognito-identity:MergeDeveloperIdentities'
                  - 'cognito-identity:UnlikeDeveloperIdentity'
                  - 'cognito-identity:UpdateIdentityPool'
                Resource: !Sub 'arn:aws:cognito-identity:${AWS::Region}:${AWS::AccountId}:identitypool/${IdentityPool}'

  CognitoESAccessRole:
    Type: 'AWS::IAM::Role'
    Properties:
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonESCognitoAccess
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service: 'es.amazonaws.com'
            Action:
              - 'sts:AssumeRole'

  # Assigns the roles to the Identity Pool
  IdentityPoolRoleMapping:
    Type: 'AWS::Cognito::IdentityPoolRoleAttachment'
    Properties:
      IdentityPoolId: !Ref IdentityPool
      Roles:
        authenticated: !GetAtt CognitoAuthorizedRole.Arn
        unauthenticated: !GetAtt CognitoUnAuthorizedRole.Arn
        
  AdminUser:
    Type: 'AWS::Cognito::UserPoolUser'
    Properties:
      DesiredDeliveryMediums:
        - 'EMAIL'
      UserAttributes:
        - Name: email
          Value: !Ref CognitoAdminEmail
      Username: !Ref CognitoAdminEmail
      UserPoolId: !Ref UserPool

  # Custom resource to configure Cognito and ES
  SetupESCognito:
    Type: 'Custom::SetupESCognito'
    Version: 1.0
    Properties:
      ServiceToken: !GetAtt LambdaESCognito.Arn
      Domain: !Ref DOMAINNAME
      CognitoDomain: !Sub ${DOMAINNAME}-${AWS::AccountId}
      UserPoolId: !Ref UserPool
      IdentityPoolId: !Ref IdentityPool
      RoleArn: !GetAtt CognitoESAccessRole.Arn

  LambdaESCognito:
    Type: 'AWS::Lambda::Function'
    Properties:
      Description: Centralized Logging - Lambda function to enable cognito authentication for kibana
      Environment:
        Variables:
          # V56536055 - 10/08/2018 - better logging capabilities
          LOG_LEVEL: 'INFO' #change to WARN, ERROR or DEBUG as needed
      Handler: index.handler
      Runtime: nodejs8.10
      Timeout: 600
      Role: !GetAtt LambdaESCognitoRole.Arn
      Code:
        S3Bucket: !Join ["-", [!FindInMap ["SourceCode", "General", "S3Bucket"], Ref: "AWS::Region"]]
        S3Key: !Join ["/", [!FindInMap ["SourceCode", "General", "KeyPrefix"],  "clog-auth.zip"]]

  LambdaESCognitoRole:
    Type: AWS::IAM::Role
    DependsOn: ElasticsearchDomain
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: root
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
          - Effect: Allow
            Action:
            - es:UpdateElasticsearchDomainConfig
            Resource: !Sub 'arn:aws:es:${AWS::Region}:${AWS::AccountId}:domain/${DOMAINNAME}'
          - Effect: Allow
            Action:
            - cognito-idp:CreateUserPoolDomain
            - cognito-idp:DeleteUserPoolDomain
            Resource: !GetAtt UserPool.Arn
          - Effect: Allow
            Action:
            - iam:PassRole
            Resource: !GetAtt CognitoESAccessRole.Arn
    
  ElasticsearchDomain: 
    Type: AWS::Elasticsearch::Domain
    Properties:
      DomainName: !Ref DOMAINNAME
      ElasticsearchVersion: "6.3"
      ElasticsearchClusterConfig: 
        InstanceCount: 1
        InstanceType: "t2.small.elasticsearch"
      EBSOptions: 
        EBSEnabled: true
        Iops: 0
        VolumeSize: 10
        VolumeType: "gp2"
      SnapshotOptions: 
        AutomatedSnapshotStartHour: 0
      AccessPolicies: 
        Version: "2012-10-17"
        Statement: 
        - Action: 'es:*'
          Principal:
            AWS: !Sub
              - arn:aws:sts::${AWS::AccountId}:assumed-role/${AuthRole}/CognitoIdentityCredentials
              - { AuthRole: !Ref CognitoAuthorizedRole }
          Effect: Allow
          Resource: !Sub 'arn:aws:es:${AWS::Region}:${AWS::AccountId}:domain/${DOMAINNAME}/*' 

  ApplyNotification:
    Type: Custom::ApplyNotification
    Properties:
      ServiceToken: !GetAtt 'ApplyBucketNotificationFunction.Arn'
      S3Bucket: !Ref 'S3'
      FunctionARN: !GetAtt 'ComprehendKeyPhraseAnalysis.Arn'
      NotificationId: S3ObjectCreatedEvent

Outputs:
  S3KeyPhraseBucket:
    Value:
      Ref: "S3"
      
  KibanaLoginURL:
    Description: Kibana login URL
    Value: !Sub https://${ElasticsearchDomain.DomainEndpoint}/_plugin/kibana/

# S3 Website URL
  WebsiteURL:
    Value: !GetAtt 
      - S3Static
      - WebsiteURL
    Description: URL for website hosted on S3
  S3BucketSecureURL:
    Value: !Join 
      - ''
      - - 'https://'
        - !GetAtt 
          - S3Static
          - DomainName
    Description: Name of S3 bucket to hold website content
